{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhdJPWqvIojX",
    "outputId": "ee8c3974-0db5-4c7c-dfe2-c41950872cb6"
   },
   "outputs": [],
   "source": [
    "! pip install lime\n",
    "! pip install tensorflow\n",
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from timeit import default_timer as timer\n",
    "from lime import lime_tabular\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import sklearn.feature_extraction\n",
    "import re\n",
    "#from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlYh0hL_Im9W"
   },
   "outputs": [],
   "source": [
    "sequence_len = 700\n",
    "total_features = 57\n",
    "amino_acid_residues = 21\n",
    "num_classes_orig = 8\n",
    "cnn_width = 17\n",
    "\n",
    "\n",
    "def get_dataset(path=\"dataset/cullpdb+profile_6133.npy\"):\n",
    "    dataset = np.load(path)\n",
    "    dataset = np.reshape(dataset, (dataset.shape[0], sequence_len, total_features))\n",
    "    ret = np.zeros((dataset.shape[0], dataset.shape[1], amino_acid_residues + num_classes_orig))\n",
    "    ret[:, :, :amino_acid_residues] = dataset[:, :, 35:56]\n",
    "    ret[:, :, amino_acid_residues:] = dataset[:, :, amino_acid_residues + 1:amino_acid_residues+ 1 + num_classes_orig]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def split_dataset(dataset, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    train_split = int(dataset.shape[0]*0.8)\n",
    "    test_val_split = int(dataset.shape[0]*0.1)\n",
    "    train = dataset[:train_split, :, :]\n",
    "    test = dataset[train_split:train_split+test_val_split, :, :]\n",
    "    validation = dataset[train_split+test_val_split:, :, :]\n",
    "\n",
    "    return train, test, validation\n",
    "\n",
    "\n",
    "def reshape_data(X, y):\n",
    "    # Reshape X\n",
    "    padding = np.zeros((X.shape[0], X.shape[2], int(cnn_width/2)))\n",
    "    X = np.dstack((padding, np.swapaxes(X, 1, 2), padding))\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "    res = np.zeros((X.shape[0], X.shape[1] - cnn_width + 1, cnn_width, amino_acid_residues))\n",
    "    for i in range(X.shape[1] - cnn_width + 1):\n",
    "        res[:, i, :, :] = X[:, i:i+cnn_width, :]\n",
    "    res = np.reshape(res, (X.shape[0]*(X.shape[1] - cnn_width + 1), cnn_width, amino_acid_residues))\n",
    "    X = res[np.count_nonzero(res, axis=(1, 2))>(int(cnn_width/2)*amino_acid_residues), :, :]\n",
    "\n",
    "    # Reshape y\n",
    "    y = np.reshape(y, (y.shape[0] * y.shape[1], y.shape[2]))\n",
    "    y = y[~np.all(y == 0, axis=1)]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_helix_labels(labels):\n",
    "\n",
    "    relevant_labels = labels[:, 3:6]\n",
    "    reshaped_labels = np.zeros(relevant_labels.shape[0])\n",
    "    reshaped_labels = np.any(relevant_labels, axis=1, out=reshaped_labels)\n",
    "\n",
    "    return reshaped_labels\n",
    "\n",
    "\n",
    "def get_alpha_helix_labels(labels):\n",
    "    return labels[:, 5]\n",
    "\n",
    "\n",
    "def get_dataset_reshaped(seed=100):\n",
    "    dataset = get_dataset('dataset/cullpdb+profile_6133_filtered.npy')\n",
    "    train, test, validation = split_dataset(dataset, seed)\n",
    "\n",
    "    X_train, y_train = train[:, :, :amino_acid_residues], train[:, :, amino_acid_residues:]\n",
    "    X_test, y_test = test[:, :, :amino_acid_residues], test[:, :, amino_acid_residues:]\n",
    "    X_val, y_val = validation[:, :, :amino_acid_residues], validation[:, :, amino_acid_residues:]\n",
    "\n",
    "    # Reshape data using the window width\n",
    "    X_train, y_train = reshape_data(X_train, y_train)\n",
    "    X_test, y_test = reshape_data(X_test, y_test)\n",
    "    X_val, y_val = reshape_data(X_val, y_val)\n",
    "\n",
    "    y_train = get_helix_labels(y_train)\n",
    "    y_test = get_helix_labels(y_test)\n",
    "    y_val = get_helix_labels(y_val)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdhhUsM8I4eN"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = get_dataset_reshaped(seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV4LYbCxI698",
    "outputId": "934325ab-f31d-405a-e937-ef584f4582a1"
   },
   "outputs": [],
   "source": [
    "do_summary = False\n",
    "\n",
    "LR = 0.0009  # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 1\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "opt = optimizers.Adam(lr=LR)\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='min')\n",
    "\n",
    "\n",
    "filepath = \"our_best_model.hdf5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "def CNN_model():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(128, 5, padding='same', activation='relu', input_shape=(cnn_width, amino_acid_residues)))\n",
    "    m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    m.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    m.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    # m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    # m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    # m.add(Dropout(drop_out))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(128, activation='relu'))\n",
    "    m.add(Dense(32, activation='relu'))\n",
    "    m.add(Dense(8, activation='relu'))\n",
    "    m.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    m.compile(optimizer=opt,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy', 'mae'])\n",
    "\n",
    "    if do_summary:\n",
    "        print(\"\\nHyper Parameters\\n\")\n",
    "        print(\"Learning Rate: \" + str(LR))\n",
    "        print(\"Drop out: \" + str(drop_out))\n",
    "        print(\"Batch dim: \" + str(batch_dim))\n",
    "        print(\"Number of epochs: \" + str(nn_epochs))\n",
    "        print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "\n",
    "        m.summary()\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_to87rGIJoe2"
   },
   "outputs": [],
   "source": [
    "net = CNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9W51mqJJuNx"
   },
   "outputs": [],
   "source": [
    "start_time = timer()\n",
    "\n",
    "history = net.fit(X_train, y_train, epochs=nn_epochs, batch_size=batch_dim, shuffle=True,\n",
    "                        validation_data=(X_val, y_val), callbacks=[checkpoint])\n",
    "\n",
    "end_time = timer()\n",
    "print(\"\\n\\nTime elapsed training the model: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_LABELS = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X', '-']\n",
    "\n",
    "ohe_text_map = {}\n",
    "def one_hot_decoder(item):\n",
    "    one_hot_labels = AA_LABELS\n",
    "    sequence = ''\n",
    "    for aa in item:\n",
    "        max_label = (0, 0)\n",
    "        for i, val in enumerate(aa):\n",
    "            if val > max_label[0]:\n",
    "                max_label = (val, i)\n",
    "        if max_label[0] != 0:\n",
    "            sequence += one_hot_labels[max_label[1]]\n",
    "    ohe_text_map[sequence] = item\n",
    "    return sequence\n",
    "\n",
    "def one_hot_decoder2(item):\n",
    "    sequence = ''\n",
    "    for aa in item:\n",
    "        if np.amax(aa) != 0:\n",
    "            index = np.argmax(aa)%21\n",
    "            print('Index: ', index)\n",
    "            sequence += AA_LABELS[index]\n",
    "            print(sequence)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_aa_data(dataset, vectorizer=sklearn.feature_extraction.text.TfidfVectorizer(lowercase=True, token_pattern=r'(?u)\\b\\w+\\b')):\n",
    "    separated_dataset = []\n",
    "    for item in dataset:\n",
    "        separated_string = ' '.join(item)\n",
    "        separated_string = re.sub(r'(\\\\ .)', r\"\\\\\\1\", separated_string)\n",
    "        separated_dataset.append(separated_string)\n",
    "    return vectorizer.fit_transform(separated_dataset)\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=True, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "\n",
    "X_train_text = [one_hot_decoder(x) for x in X_train]\n",
    "X_test_text = [one_hot_decoder(x) for x in X_test]\n",
    "train_vectors = tokenize_aa_data(X_train_text, vectorizer)\n",
    "test_vectors = tokenize_aa_data(X_test_text, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train_text = [one_hot_decoder(y, x=False) for y in y_train]\n",
    "#Y_train_text = one_hot_decoder(y_train, x=False)\n",
    "#Y_test_text = one_hot_decoder(y_test, x=False)\n",
    "\n",
    "#Y_test_text = [one_hot_decoder(y, x=False) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(item):\n",
    "    # Item received should be a single amino-acid sequence\n",
    "    try:\n",
    "        encoded_item = []\n",
    "        length_of_padding= 17-len(item)\n",
    "        for c in item:\n",
    "            encoded_char = np.zeros(21)\n",
    "            encoded_char[AA_LABELS.index(c)] = 1\n",
    "            encoded_item.append(encoded_char)\n",
    "        for i in range(length_of_padding):\n",
    "            encoded_item.append(np.zeros(21))\n",
    "        encoded_item = np.concatenate(encoded_item, axis=0 )\n",
    "        encoded_item = encoded_item.reshape(1,17,21)\n",
    "        return encoded_item\n",
    "    except:\n",
    "        print(item)\n",
    "        \n",
    "def predict_text_to_ohe(items):\n",
    "    #item_ohe = ohe_text_map[item[0]]\n",
    "    return net.predict([one_hot_encode(item) for item in items])\n",
    "    #return net.predict(X_test)[idx]\n",
    "\n",
    "#print(one_hot_encode(one_hot_decoder(X_test[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_characters(item):\n",
    "    separated_string = ' '.join(item)\n",
    "    separated_string = re.sub(r'(\\\\ .)', r\"\\\\\\1\", separated_string)\n",
    "    print(separated_string)\n",
    "    return separated_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"sequential\" expects 1 input(s), but it received 14 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:10' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:11' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:12' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:13' shape=(None, 17, 21) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredict_text_to_ohe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_text\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [35], line 20\u001b[0m, in \u001b[0;36mpredict_text_to_ohe\u001b[1;34m(items)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_text_to_ohe\u001b[39m(items):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#item_ohe = ohe_text_map[item[0]]\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mone_hot_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileq9cqxe2w.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"sequential\" expects 1 input(s), but it received 14 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:10' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:11' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:12' shape=(None, 17, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:13' shape=(None, 17, 21) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "predict_text_to_ohe(X_test_text[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [41], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m explainer \u001b[38;5;241m=\u001b[39m LimeTextExplainer(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m17\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#X_train_text,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#training_labels = y_train,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#feature_names = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X', '-'], \u001b[39;00m\n\u001b[0;32m      6\u001b[0m     split_expression\u001b[38;5;241m=\u001b[39msplit_characters,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#bow being false replaces the removed feature with 'UNKWORDZ' instead of removing it,\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#It should be false because character order matters for us, but this breaks our encoder. We should use 'X' instead of 'UNKWORDZ'\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#bow=False,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot helix\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 11\u001b[0m     char_level\u001b[38;5;241m=\u001b[39m\u001b[43mtrue\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#array = one_hot_encode('AKHESLSHFDAMWF')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#print(array)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#new_arry = np.concatenate(array, axis=0 )\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#predict_text_to_ohe(X_test_text[idx])\u001b[39;00m\n\u001b[0;32m     25\u001b[0m exp \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(X_test_text[idx], predict_text_to_ohe, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m17\u001b[39m) \u001b[38;5;66;03m#, labels=(1,))# labels=[0, 17])\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "explainer = LimeTextExplainer(\n",
    "    17,\n",
    "    #X_train_text,\n",
    "    #training_labels = y_train,\n",
    "    #feature_names = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X', '-'], \n",
    "    split_expression=split_characters,\n",
    "    #bow being false replaces the removed feature with 'UNKWORDZ' instead of removing it,\n",
    "    #It should be false because character order matters for us, but this breaks our encoder. We should use 'X' instead of 'UNKWORDZ'\n",
    "    #bow=False,\n",
    "    class_names=['Helix', 'Not helix'],\n",
    "    char_level=true)\n",
    "\n",
    "#array = one_hot_encode('AKHESLSHFDAMWF')\n",
    "#print(array)\n",
    "#new_arry = np.concatenate(array, axis=0 )\n",
    "#print(new_arry.reshape(17,21))\n",
    "\n",
    "#print(one_hot_encode('AKHESLSHFDAMWF'))\n",
    "#print(one_hot_encode(X_test_text[idx])[0])\n",
    "#print(X_test_text[idx])\n",
    "#print(X_test[0])\n",
    "#predict_text_to_ohe('AKHESLSHFDAMWF')\n",
    "\n",
    "#predict_text_to_ohe(X_test_text[idx])\n",
    "exp = explainer.explain_instance(X_test_text[idx], predict_text_to_ohe, num_features=17) #, labels=(1,))# labels=[0, 17])\n",
    "\n",
    "#exp.show_in_notebook()\n",
    "#print('Document id: %d' % idx)\n",
    "#print('Predicted class =', net.predict(X_test[idx]))\n",
    "#print('True class: ', y_test[idx])\n",
    "\n",
    "#print('Explanation for class %s' % class_names[0])\n",
    "#print('\\n'.join(map(str, exp.as_list(label=0))))\n",
    "\n",
    "#exp = explainer.explain_instance(X_test_text[idx], c.predict_proba, num_features=6, top_labels=2)\n",
    "#print(exp.available_labels())\n",
    "#exp.show_in_notebook(text=False)\n",
    "\n",
    "#exp.show_in_notebook(text=X_test_text[idx], labels=(0,))\n",
    "#exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_hot_encode('AKHESLSHFDAMWF'))\n",
    "'''\n",
    "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "'''\n",
    "\n",
    "print(X_test_text[idx])\n",
    "'''\n",
    "AKHESLSHFDAMWF\n",
    "'''\n",
    "\n",
    "print(one_hot_decoder(X_test[idx]))\n",
    "\n",
    "'''\n",
    "AKHESLSHFDAMWF\n",
    "'''\n",
    "print(X_test.shape)\n",
    "\n",
    "#print(one_hot_decoder2(21*[X_test[8]]))\n",
    "\n",
    "'''\n",
    "DDDDDDDDDDDDDDDDDDDDD\n",
    "'''\n",
    "\n",
    "#predict_text_to_ohe('AKHESLSHFDAMWF')\n",
    "\n",
    "#print(predict_text_to_ohe)\n",
    "\n",
    "#print(one_hot_decoder2(X_test[8]))\n",
    "#print(predict_text_to_ohe(one_hot_decoder2(X_test[8])))\n",
    "\n",
    "#print(np.amax(X_test[8][0]))\n",
    "#(X_test[8][0])\n",
    "\n",
    "#predict_text_to_ohe(\n",
    "\n",
    "#one_hot_encode('AKHESLSHFDAMWFEFN')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdRqOcl7Jyd5"
   },
   "outputs": [],
   "source": [
    "explainer = lime_tabular.RecurrentTabularExplainer(\n",
    "    X_train, training_labels = y_train,\n",
    "    feature_names = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X', '-'],\n",
    "    class_names = ['Helix', 'Not helix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Yeah, got the first part!')\n",
    "exp1 = explainer.explain_instance(X_test[10], net.predict, num_features=10, labels=(0,))\n",
    "exp1.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2 = explainer.explain_instance(X_test[10], net.predict, num_features=10, labels=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.show_in_notebook(text=X_test[idx], labels=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
